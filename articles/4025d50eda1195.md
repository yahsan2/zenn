---
title: "OpenAI Agents SDKの仕組みを調査してみた"
emoji: "😎"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["OpenAI", "Python", "AI", "LLM", "エージェント"]
published: false
---
## OpenAI Agents SDKとは

[OpenAI Agents SDK (openai-agents-python)](https://github.com/openai/openai-agents-python)は、大規模言語モデル(LLM)を「エージェント」としてツールや他のエージェントと組み合わせ、マルチエージェントのワークフローを構築するための軽量フレームワークです。最小限のプリミティブ（基礎要素）によって構成されており、具体的には以下の3つが中核となります。

- [Agents（エージェント）](https://github.com/openai/openai-agents-python/blob/main/src/agents/agent.py)
- [Handoffs（ハンドオフ）](https://github.com/openai/openai-agents-python/blob/main/src/agents/handoffs.py)
- [Guardrails（ガードレール）](https://github.com/openai/openai-agents-python/blob/main/src/agents/guardrail.py)

これにより、Pythonコード主体で柔軟にエージェント同士やツールの連携を表現でき、複雑なAIワークフローを簡潔に実装できます。以下では、リポジトリ内の構成要素について詳しく解説します。

## コアのエージェント実装の詳細

エージェント (Agent) はLLMに指示（システムプロンプト）やツール利用権限などを与えて特定の役割を持たせたものです。エージェントはPythonのデータクラス(@dataclass)として定義されており、主要フィールドに以下があります。

- **name**: エージェントの名前です。
- **instructions**: エージェントに与える指示文（システムメッセージに相当）で、文字列あるいは動的に指示文を生成する関数を渡せます。
- **model**: 利用するモデルを指定します。指定しない場合はデフォルトのモデル設定が使われます。モデル固有のパラメータ（温度やトップPなど）は `model_settings` で調整できます。
- **tools**: エージェントが呼び出せるツール（関数）のリストです。これによりエージェントは外部機能を関数経由で利用できます。
- **handoffs**: このエージェントから委譲できるサブエージェントのリストです。後述するHandoff機能で使用します。
- **input_guardrails / output_guardrails**: 入力や出力に対するガードレール（検証ルール）をリストで指定できます。
- **hooks**: エージェントのライフサイクルイベント（ツール実行前後など）に対するフックを定義できます。

エージェントの実行はエージェントループと呼ばれる仕組みで管理されます。OpenAI Agents SDKには対話の流れを自動で処理するループ処理が組み込まれており、開発者は単にエージェントと初期入力を与えるだけで、以下のようなステップが順次行われます。

1. **LLM呼び出し**: 現在のエージェントと入力（ユーザーメッセージ）からプロンプトを構築し、LLMモデルを呼び出します。このときエージェントのinstructionsがシステムメッセージとして設定され、必要に応じてこれまでの対話履歴（Itemsと呼ばれるメッセージ列）も含まれます。
2. **LLMの応答処理**: モデルの応答内容を解析します。応答が直接最終アウトプット（ユーザーへの答え）である場合はループを終了し、結果を返します。一方、応答内にツール呼び出し（関数の実行要求）が含まれる場合、そのツールを実行し、その結果を会話に追加して再度LLMを呼び出します。また、応答としてハンドオフの指示（別のエージェントへの委譲）があった場合は、現在のエージェントと入力をその指定されたエージェントに切り替え、改めてその新しいエージェントでLLM呼び出しを行います。これらの場合、ループは継続され、再びステップ1に戻ります。
3. **ガードレールと例外**: エージェント実行中、並行して入力ガードレール（input_guardrails）がチェックされており、もし不適切な入力でエージェントを止める必要があれば早期に中断します。最終応答生成後には出力ガードレール（output_guardrails）による検証が行われ、基準を満たさない場合はエラーが投げられたり安全な出力に置き換えられます。さらに無限ループ防止のため、対話ターン数が指定した上限を超えた場合には例外 (MaxTurnsExceeded) が発生します。

このようにAgentはシステムプロンプト・ツール・ガードレールなどを備えたLLMコンポーネントであり、[Runner](https://github.com/openai/openai-agents-python/blob/main/src/agents/run.py)（後述）が内部で上記ループ処理を実装することで、ツール実行や他エージェント委譲を織り交ぜた対話を自動的に進行します。開発者は必要な構成要素を定義して `Runner.run()` に渡すだけで、あとはSDKがエージェントの思考・行動サイクルを管理します。

## 主要なクラスやモジュールの役割

リポジトリの `src/agents` 配下には、Agents SDKの中核をなすクラスやモジュールが整理されています。その主な役割は以下の通りです。

- **[Agent クラス](https://github.com/openai/openai-agents-python/blob/main/src/agents/agent.py)**: 上述の通り、エージェントを表すデータクラスです。LLMに与える振る舞い（名前、説明、指示）や利用できるツール・サブエージェント・ガードレール等をプロパティとして持ちます。システム上は会話中の「エージェント」を構成する設定オブジェクトといえます。

- **[Runner クラス](https://github.com/openai/openai-agents-python/blob/main/src/agents/run.py)**: エージェントを実行するためのランナーです。`Runner.run(agent, input, ...)` のように使用し、非同期でエージェントループを開始して `RunResult`（実行結果）を返します。`run_sync` や `run_streamed` といったメソッドも提供され、同期実行やストリーミング実行（トークン逐次出力）にも対応しています。Runner内部で前述のループ処理（LLM呼び出し→ツール実行→ハンドオフ）が実装されており、ワークフロー全体のオーケストレーションを担います。

- **[Tool / FunctionTool クラス](https://github.com/openai/openai-agents-python/blob/main/src/agents/tool.py)**: エージェントが呼び出せる外部関数（ツール）を表すクラスです。Pythonの関数をそのままツールに変換できるユーティリティデコレータ `@function_tool` が用意されており、関数名・引数から自動的にJSONスキーマを生成し、LLMにとって呼び出し可能な「関数」として登録します。例えば以下のように関数にデコレータを付与するだけでツール化が可能です。

```python
@function_tool
def get_weather(city: str) -> str:
    # ...天気取得処理...
    return f"The weather in {city} is sunny"
```

こうして得られたツール（FunctionTool）は、Agentのtoolsリストに追加して利用します。Agents SDKにはカスタム関数のほか、組み込みのツール実装も含まれており、たとえば[FileSearchTool（ファイル検索）](https://github.com/openai/openai-agents-python/blob/main/examples/tools/file_search.py)、[WebSearchTool（ウェブ検索）](https://github.com/openai/openai-agents-python/blob/main/examples/tools/web_search.py)、[ComputerTool（コード実行環境）](https://github.com/openai/openai-agents-python/blob/main/examples/tools/computer_use.py)などが提供されています。ツール呼び出し時の入出力は自動的に検証・シリアライズされ、OpenAIの「関数呼び出し(Function Calling)」フォーマットでLLMとやり取りされます。

- **[Handoff クラス](https://github.com/openai/openai-agents-python/blob/main/src/agents/handoffs.py)**: エージェントから他のエージェントへの委譲を表すデータクラスです。あるエージェントが特定の条件で別エージェントに処理を引き継ぐ際、このHandoffが内部的にツールとしてモデルに提示されます（例: "Refund Agent"へのハンドオフはツール名 `transfer_to_refund_agent` としてLLMに認識される）。エージェントのhandoffsリストに他のAgentやHandoffオブジェクトを指定することで設定でき、実行時にLLMがその関数（Handoffツール）を呼び出すと委譲が発生します。`agents.handoff()` ヘルパー関数でHandoffを作成でき、ツール名や説明の上書き、ハンドオフ時のコールバック（on_handoff）、次エージェントへの入力変換（input_filter）など細かな設定も可能です。

- **[Guardrail クラス](https://github.com/openai/openai-agents-python/blob/main/src/agents/guardrail.py)**: 入出力の安全性や整合性をチェックする仕組みです。InputGuardrail と OutputGuardrail の2種があり、それぞれエージェント実行前のユーザー入力検証と、最終出力の検証を行います。Guardrailは関数や正規表現によるチェック、カスタムロジックなどを実装したクラスで、AgentやRunnerのパラメータとしてリスト指定します。実行中にガードレール違反が検出されると、その場で処理を中断したりエラーを投げることで不適切な動作を防ぎます。例えば「ユーザー入力が長すぎる場合は打ち切る」「出力に機密情報が含まれていないか確認する」といった用途に利用できます。

- **[RunResult クラス](https://github.com/openai/openai-agents-python/blob/main/src/agents/run.py)**: Runnerの実行結果を表すオブジェクトです。`result.final_output` で最終的なエージェントの回答を取得できるほか、`result.new_items` としてエージェントが生成したすべてのメッセージやツール実行結果のリスト（Itemの一覧）を保持します。これにより対話の履歴や各ステップの詳細（どのツールが何を返したかなど）を確認することができます。ストリーミング実行した場合は `RunResultStreaming` を受け取り、逐次イベントを消費しつつ最終結果も取得できます。

- **[Item モジュール](https://github.com/openai/openai-agents-python/blob/main/src/agents/items.py)**: 内部で用いられる会話項目の定義です。各エージェント対話の構成要素（ユーザーからのメッセージ、エージェントの回答、ツール呼び出し要求、ツールの応答、ハンドオフの発生など）をアイテムとして表現します。例えば「ユーザー入力」アイテム、「アシスタント（エージェント）メッセージ」アイテム、「関数コール」アイテムといったクラスが含まれ、RunResult経由でこれら履歴にアクセスできます。開発者は通常これら内部実装を意識する必要はありませんが、デバッグやトレース時に役立ちます。

- **モデル関連 ([agents.models](https://github.com/openai/openai-agents-python/tree/main/src/agents/models))**: LLMモデルを呼び出すためのインターフェースと実装が含まれます。Model 抽象クラス (interface) では、モデルへの入力メッセージやツール定義を渡して結果を取得するメソッドが定義されています。デフォルト実装として OpenAI のAPIを使う [OpenAIChatCompletionsModel](https://github.com/openai/openai-agents-python/blob/main/src/agents/models/openai_chatcompletions.py) や [OpenAIResponsesModel](https://github.com/openai/openai-agents-python/blob/main/src/agents/models/openai_responses.py) が用意されており、それぞれChat Completions APIを用いるか、別の応答フォーマットを用いるかの違いがあります。`OpenAIChatCompletionsModel` は関数呼び出しや多段のメッセージ履歴に対応しており、`OpenAIResponsesModel` はシンプルなテキスト出力向けに使われます（2種類のモデル形態で利用可能な機能が異なるため、ワークフロー内で混在させないことが推奨されています）。また `AsyncOpenAI` という非同期クライアントも提供され、OpenAIのREST APIへのリクエスト送信を担います。さらに `ModelProvider` インターフェースにより、モデル名から適切なModelインスタンスを提供する仕組みもあります。これらを拡張・実装することで、OpenAI以外のLLMサービスやローカルモデルをSDKに組み込むことも可能です。

- **[トレーシング (agents.tracing)](https://github.com/openai/openai-agents-python/tree/main/src/agents/tracing)**: エージェントの動作を追跡・可視化するためのモジュールです。トレース機能を有効にすると、各エージェント呼び出しやツール実行、ハンドオフといったイベントがログ（Span）として記録されます。`Runner.run()` の引数や環境変数でトレースを有効/無効にしたり、トレースIDやメタデータを付与できます。収集されたトレースは開発中のデバッグやフローの可視化に役立つほか、OpenAI提供の評価・フィードバック基盤と連携してモデルのチューニングに活用することも想定されています。

以上が主なクラス・モジュールの概要です。これらはすべてopenai-agents-pythonリポジトリ内でディレクトリ構成的にも整理されており、例えば `agents/agent.py` にAgentクラス、`agents/run.py` にRunnerクラス、`agents/tool.py` にツール関連の実装、`agents/handoffs.py` にハンドオフ関連実装、といった形でモジュールが分割されています（テストコードは `tests/` 以下、公式ドキュメントは `docs/` 以下に配置されています）。

## 依存ライブラリや技術スタック

Agents SDKはPythonベースで構築されており、AsyncIOを活用して非同期処理（LLMへのリクエストやツール実行の待機など）を効率的に行います。主な依存ライブラリ・技術要素は次の通りです。

- **[OpenAI API](https://platform.openai.com/docs/api-reference)**: デフォルトのモデル実装はOpenAIのAPI（Chat Completionsエンドポイントなど）を使用します。OpenAI提供のPythonクライアントを使う代わりに、SDK内でHTTPクライアント（後述）を用いて直接REST APIを叩く非同期実装になっています。環境変数や`AsyncOpenAI`クライアントを通じてAPIキーやエンドポイントURLを設定可能で、OpenAI互換のAPI（ローカルLLMサーバー等）にも接続できます。

- **[HTTPX](https://www.python-httpx.org/)**: 非同期HTTP通信ライブラリで、OpenAI APIへのリクエスト送信に使用されています。`AsyncOpenAI` クラスの内部で `httpx.AsyncClient` を利用し、タイムアウトや接続管理を行っています。これによりイベントループをブロックせず複数のエージェントやツール呼び出しを同時並行で扱えます。

- **[Pydantic](https://pydantic-docs.helpmanual.io/)**: データモデルの検証ライブラリで、特にFunctionToolの引数検証やスキーマ生成に使われています。任意のPython関数のシグネチャからJSONスキーマを起こし、LLMに提供する関数定義に変換する際や、LLMから返ってきた関数引数（JSON）をPython型に復元する際にPydanticのバリデーションが働きます。さらにAgentのoutput_typeにPydanticのモデルクラスを指定すれば、モデルの出力を直接そのデータクラスに構造化することも可能です。SDK内部ではPydanticのTypeAdapterを用いて、通常のdataclassやTypedDictなども一貫した方法でシリアライズ/デシリアライズできるよう工夫されています。

- **その他のライブラリ/技術**: ロギング（標準loggingモジュール）やdataclasses（標準ライブラリ）を活用しています。組み込みツールの実装によっては追加の依存がある場合もあります（例えばWebSearchToolは外部の検索APIを叩く可能性があります）。ドキュメンテーションサイトはMkDocsで構築されており、リポジトリにはその設定(`mkdocs.yml`)やMarkdown形式のドキュメントが含まれています。テストはpytestベースで書かれており、examplesディレクトリには各機能を示すスクリプト（エージェントのパターン例、カスタムモデルプロバイダ例など）が配置されています。

以上のように、Agents SDKはシンプルなPythonスタック上に構築されており、余分な抽象化を増やすことなく言語機能や実績あるライブラリ（dataclassesやPydanticなど）で堅実に実装されています。そのため、開発者はPythonコードを書く感覚でエージェントやツールを定義でき、必要であれば低レベルの挙動（HTTP通信やデータ検証）のカスタマイズも行いやすくなっています。

## 設計パターンや拡張ポイント

OpenAI Agents SDKの設計は「シンプルな構成要素を組み合わせて高度な振る舞いを実現する」ことを志向しています。大げさなフレームワークというよりは、小さなプラグイン可能コンポーネントの集まりとしてデザインされており、開発者自身がカスタムエージェント機構を構築しやすいよう考慮されています。以下に主な設計上の特徴と拡張ポイントをまとめます。

- **最小限のプリミティブとコンポジション**: 前述の通りAgent・Tool・Handoff・Guardrailといった最小要素を組み合わせることで、多様なフローを表現します。これはコンポジション（合成）による設計パターンと言えます。例えば、「言語判定エージェント」がユーザー発話を解析して適切な「英語回答エージェント」または「スペイン語回答エージェント」にハンドオフする、といったエージェント同士の委譲関係も、各Agentのhandoffsに別Agentを登録するだけで構築できます。エージェント間の制御移譲はHandoffを通じた委譲パターンで実現され、処理の分担や責務の分離をわかりやすくモデル化できます。

- **関数呼び出しによる拡張（コマンドパターン）**: ツール（FunctionTool）は、エージェントから呼び出される外部コマンドとして機能します。LLMが関数を呼び出すたびにSDK側で対応するPython関数が実行され、その結果が再度LLMにフィードバックされます。この一連の流れは、LLMが自律的に外部コマンドを実行してタスク達成に近づくエージェントループの原動力であり、デザインパターン的にはAIによるコマンド実行と結果反映のサイクルです。開発者は任意の関数をツールとして追加できるため（＝拡張ポイント）、API呼び出しや計算、データベース操作など様々な動作をエージェントに習得させることができます。これはLangChainなどのフレームワークにおける「ツール追加」と同様の拡張性ですが、本SDKではPydanticにより引数の型安全性も担保される点が特徴です。

- **依存性の注入 (Dependency Injection)**: SDKではコンテキスト (context) オブジェクトを通じて依存関係や状態をエージェント実行に渡すデザインが取られています。エージェントクラスはジェネリック型になっており、コンテキストとなる独自クラスを型引数として指定できます。実行時に開発者が用意したコンテキスト（例えばユーザーIDやデータベース接続を持つオブジェクト）が `Runner.run(agent, input, context=my_context)` で渡されると、各ツール関数やガードレールはそのコンテキストにアクセスできます。これによりグローバル変数に頼らず必要情報をエージェント内に供給でき、テストもしやすくなっています。コンテキストは任意クラスでよいため、Pydanticモデルやdataclass、シンプルな辞書など好きな形で状態管理を実装できる柔軟な拡張ポイントです。

- **モデル切替とプロバイダ差し替え**: Agents SDKは特定のLLM実装にロックインしないよう配慮されています。Agentごとに個別のmodelを指定可能なほか、`Runner.run()` 呼び出し時に `RunConfig` で全エージェント共通のモデル指定やプロバイダ切替ができます。`ModelProvider` インターフェースを実装すれば、与えられた名前を任意のモデルクラスにマップできるため、例えば「社内のGPT-4 APIを使う」「ローカルのLLMサーバを使う」といった拡張が可能です。デザインパターン的にはStrategyパターンに近く、モデル呼び出し部分の実装を差し替えてもAgentやToolのロジックはそのまま動作します。

- **フックとライフサイクル管理**: Agentにはhooksプロパティがあり、AgentHooksクラスを通じてエージェントのライフサイクルイベントにコールバックを仕込めます。例えば「LLMへのプロンプト送信前」「LLMから応答を受け取った後」「ツール実行前後」「ハンドオフ時」「エージェント完了時」などのイベントで独自処理（ロギングや結果の加工）を差し込むことができます。これはオブザーバーパターン的な拡張ポイントで、SDKのデフォルト動作を上書きすることなく追加処理を挿入できます。エージェント単位だけでなくRunner全体のイベントについてもトレース機能やカスタムイベントハンドラで監視・拡張でき、開発者は必要に応じて内部挙動を細かく追跡できます。

- **トレーシングと評価**: 内部の[tracingモジュール](https://github.com/openai/openai-agents-python/tree/main/src/agents/tracing)は、エージェントの実行履歴をオープンな形式で記録し、外部システムとの連携を可能にします。例えば社内ツールで実行ログを可視化したり、OpenAIの評価APIにこのトレースを渡して対話の品質評価やフィードバックループを構築する、といった応用が考えられています。トレーシングは開発者の負担なく自動で行われ、必要に応じて無効化もできるクロスカッティングな拡張ポイントです（性能に影響を与えない限り有効にしておくとデバッグに有用です）。

まとめると、本SDKはシンプルな構成要素（エージェント、ツール、ハンドオフ等）をベースに、デザインパターンとしては「責務の委譲（ハンドオフ）」「コマンド実行（ツール）」「依存性注入（コンテキスト）」「戦略の切替（モデルプロバイダ）」などを活用しています。拡張ポイントも豊富で、新しいツールの追加や独自モデルサービスの利用、カスタム検証ロジックの適用、実行フローへのフック追加といった柔軟なカスタマイズが可能です。既定の挙動はシンプルですが、必要に応じて内部コンポーネントを差し替えることで、自作のエージェント機構にも対応できる設計になっています。

## 実際のワークフローの流れ

最後に、OpenAI Agents SDKを用いてエージェントを実行する際の具体的なフローを、コード上の観点から追ってみます。開発者が行う典型的な手順と、それに伴うSDK内部の処理は以下のようになります。

1. **エージェントとツールの定義**: まずPythonコード上でAgentインスタンスを作成します。例えば名前と指示を与えたシンプルなエージェントを定義し、必要に応じてツール関数を `@function_tool` でデコレートして登録します。複数のエージェントを組み合わせる場合は、あるAgentのhandoffsに別のAgentを渡しておきます（必要なら `handoff()` 関数で詳細設定したHandoffオブジェクトを渡す）。またグローバルな設定として、使用するモデル（例: "gpt-3.5-turbo"）やパラメータ（温度など）をAgentごと、または後でRunner実行時にRunConfigで指定します。

2. **[Runnerの実行呼び出し](https://github.com/openai/openai-agents-python/blob/main/src/agents/run.py)**: 準備ができたら、`Runner.run(agent, input, context=...)` を呼び出します（同期ブロッキングで良い場合は `run_sync` も利用可）。ここでinputはユーザーからの質問など文字列、もしくは過去の対話履歴（Itemのリスト）を渡せます。contextには任意のコンテキストオブジェクトを渡し、無ければデフォルトで空のコンテキストが生成されます。Runnerはまず初期化処理として、与えられたAgent（開始エージェント）と入力からプロンプトを構築します。プロンプトにはエージェントのinstructionsがシステムメッセージとして設定され、ユーザー入力はユーザーメッセージとして組み立てられます。この際、Agentにツールが登録されていればOpenAI APIの関数定義パラメータにそれらの関数スキーマを渡します。ハンドオフ先のエージェントがあれば、それも内部では特殊なツール（例えば `transfer_to_x` 関数）として関数定義一覧に加えられます。ガードレールが指定されていれば、このタイミングで入力ガードレールのチェックを非同期で仕掛け、LLM呼び出しと並行して検証を走らせます。

3. **LLMへの問い合わせ**: 構築したメッセージ（と関数定義）をモデルAPIに送信し、LLMから応答を取得します。OpenAIのChat Completionsの場合、モデルがツールを呼び出したい場合は応答として関数コール（function_callフィールド）が返され、直接回答を出す場合はassistantメッセージとしてテキストが返ります。SDKはこの応答を解析し、RunResult内の新たなItem（メッセージや関数コール）として追加します。

4. **モデル応答の処理**: 応答内容に応じて分岐します。
   - **最終出力の場合**: モデルがユーザーへの回答を最終結果として返した場合（例えばassistant役割で適切な回答テキストを出力し、function_callが無い場合）、エージェントループを終了します。RunResultにその出力が格納され、これを `final_output` 属性から取得できます。Agentのoutput_typeが指定されている場合、返答テキストはPydanticのパーサーで構造化データに変換され、指定型のオブジェクトとして `final_output` にセットされます。出力ガードレールが設定されていれば、この時点で最終出力に対し全チェックを実行し、問題なければ結果を確定します。
   - **ツール呼び出し要求の場合**: モデル応答が関数呼び出し（tool実行）の指示だった場合、Runnerはその内容を読み取り対応するPython関数を実行します。例えば `get_weather` というツールを引数 `{"city": "Tokyo"}` で呼び出そうとした場合、SDKは内部で `get_weather("Tokyo")` を実行し、その返り値（天気情報の文字列など）を取得します。実行結果は新たなassistantメッセージ（ツールからの応答をLLM視点ではAssistantの発話として渡す）として対話履歴に追加されます。そして改めてその最新の履歴を入力としてLLM APIを呼び直し、次の応答を待ちます。このようにモデル→ツール→モデルと対話が続くことで、エージェントは外部機能を使いこなしながら質問への解答を組み立てていきます。ツール実行中にエラーが起これば既定ではそのエラー内容が簡単なメッセージとして返り、エージェントはそれを踏まえて別の対応を試みるか最終回答に切り替えます（必要なら `default_tool_error_function` をオーバーライドしてカスタムエラー処理も可能です）。
   - **ハンドオフ要求の場合**: モデル応答が別エージェントへの委譲（Handoffツールの呼び出し）であった場合、Runnerはその呼び出し先となるエージェントを特定します。例えば応答が関数呼び出し `{"name": "transfer_to_refund_agent"}` だった場合、対応するRefund Agentが事前にhandoffsに登録されていれば、現在の対話コンテキストを引き継ぎつつ現在のエージェントをRefund Agentに切り替えます。切り替え時には、元のエージェントでの直近のユーザー入力やモデル出力を新しいエージェントに入力として渡す挙動になります（必要に応じてhandoff定義時にinput_filterで渡す内容を調整可能）。ハンドオフ時点で指定されたon_handoffコールバックがあればそれも実行し、副次的な処理（例えばデータベースから関連情報を事前取得してコンテキストに格納する等）を行います。こうしてエージェントが切り替わった後、再度ステップ3（LLMへの問い合わせ）に戻り、今度は新しいエージェントの指示・ツールに基づいて対話が続行されます。ハンドオフは必要に応じて何度も発生しうるため、エージェント間でタスクがリレーされ最終的な回答に至ります。

5. **ループ継続と終了**: 上記のモデル応答→処理→再問い合わせのループは、いずれモデルが最終アウトプットを返すまで続きます。あるいは設定された最大ターン数に達した場合は強制終了して例外を投げます。通常は数ターンのうちにユーザーの質問に答える形で完了し、Runnerは収集した全てのItems（メッセージ履歴やツール結果）と最終出力を含むRunResultオブジェクトを呼び出し元に返します。

6. **結果の活用**: 開発者は受け取ったRunResultから `final_output` を取り出してユーザーに返答したり、 `new_items` を参照して対話過程をログに記録したりできます。必要ならusage情報（トークン使用量など）がRunResultに含まれているためコスト計測も可能です（OpenAI API利用時はAPIレスポンス由来のトークン数を自動集計）。

7. **トレースとモニタリング**: トレース機能が有効な場合、実行中の各イベント（LLM呼び出し開始・終了、ツール実行開始・終了、ハンドオフ発生、ガードレール違反など）が逐次記録されています。`Runner.run()`完了時には一連のTraceが完成しており、開発中であればこれを専用ビューワーで可視化したり、ログとして保存して後から分析できます。ストリーミング実行を用いた場合は、RunResultStreamingオブジェクトから `.stream_events()` でトークンごとの部分出力イベントを購読できるため、ユーザーにトークン単位で表示しつつバックエンドでは最終結果を組み立てるといった高度な応用もできます。

以上がOpenAI Agents SDKにおける典型的なワークフローの流れです。エージェント定義→Runner実行→LLMとツールのループ→結果取得という一連の処理はSDKによって抽象化されていますが、その内部では上記のようにLLM APIの機能（メッセージ・関数コール・ストリーム）を駆使し、Python側で関数実行やエージェント切替などのロジックを順次適用しています。リポジトリのコード（特に [agents/run.py](https://github.com/openai/openai-agents-python/blob/main/src/agents/run.py) のRunner実装）を読むと、非同期タスクの管理や例外処理の詳細まで把握でき、自作の発展的なエージェントシステムを構築する際の参考になるでしょう。公式ドキュメントの[Examplesフォルダ](https://github.com/openai/openai-agents-python/tree/main/examples)には様々なパターンのエージェント実装例（言語 triage エージェント、ツールを使った計算エージェント、カスタムモデルプロバイダの利用例など）が含まれているため、そちらも実践的なガイドとして役立ちます。
